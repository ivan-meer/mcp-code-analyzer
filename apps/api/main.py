from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any


import os
import ast # For Python AST parsing
import json
import sqlite3
from pathlib import Path
import uvicorn
import asyncio
import logging
import uuid
import traceback
import time
from datetime import datetime, timezone
import re # For JSDoc regex parsing

# üöÄ –ò–º–ø–æ—Ä—Ç AI —Å–µ—Ä–≤–∏—Å–æ–≤
from ai_services import (
    initialize_ai_services, 
    get_ai_manager, 
    CodeContext, 
    AIResponse,
    AIServiceError
)

# üîç –ò–º–ø–æ—Ä—Ç –Ω–∞—à–µ–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
from monitoring_system import (
    analytics_logger,
    track_analysis_operation,
    log_analysis_event,
    get_system_health,
    get_analytics_summary,
    EventType
)


from dotenv import load_dotenv
load_dotenv()



# –ú–æ–¥–µ–ª–∏ –¥–∞–Ω–Ω—ã—Ö

# Documentation Models
class DocFunctionParam(BaseModel):
    name: str
    type: Optional[str] = None
    description: Optional[str] = None

class DocFunction(BaseModel):
    name: str
    description: Optional[str] = None
    params: List[DocFunctionParam] = Field(default_factory=list)
    returns: Optional[Dict[str, Optional[str]]] = None # e.g., {"type": "str", "description": "..."}
    line_start: Optional[int] = None
    line_end: Optional[int] = None

class DocFile(BaseModel):
    file_path: str
    functions: List[DocFunction] = Field(default_factory=list)

# Main Models
class ProjectAnalysisRequest(BaseModel):
    path: str
    include_tests: bool = True
    analysis_depth: str = "medium"

class FileInfo(BaseModel):
    path: str
    name: str
    type: str
    size: int
    lines_of_code: Optional[int] = None
    functions: List[str] = [] # Still keep simple list of function names for other uses
    imports: List[str] = []
    todos: Optional[List[Dict[str, Any]]] = Field(default_factory=list)
    doc_details: Optional[List[DocFunction]] = Field(default_factory=list) # Parsed function documentation

class ProjectAnalysisResult(BaseModel):
    project_path: str
    files: List[FileInfo]
    dependencies: List[Dict[str, Any]]
    metrics: Dict[str, Any]
    architecture_patterns: List[str]
    all_todos: List[Dict[str, Any]] = Field(default_factory=list)
    project_documentation: Optional[List[DocFile]] = Field(default_factory=list)

class CodeExplanationRequest(BaseModel):
    code: str
    language: str = "javascript"
    level: str = "intermediate"
    file_path: Optional[str] = None
    project_context: Optional[Dict[str, Any]] = None

class CodeExplanation(BaseModel):
    explanation: str
    concepts: List[str]
    examples: List[str]
    recommendations: List[str]
    improvements: List[str] = []
    patterns: List[str] = []
    confidence_score: float = 0.0
    ai_provider: str = "unknown"

class ComprehensiveAnalysisRequest(BaseModel):
    file_path: str
    project_path: str
    explanation_level: str = "intermediate"

class ComprehensiveAnalysisResult(BaseModel):
    explanation: Optional[Dict[str, Any]] = None
    improvements: List[str] = []
    patterns: List[str] = []
    analysis_metadata: Dict[str, Any] = {}

# –ì–ª–æ–±–∞–ª—å–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è AI –º–µ–Ω–µ–¥–∂–µ—Ä–∞
ai_manager = None

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è FastAPI
app = FastAPI(
    title="MCP Code Analyzer API with AI Integration",
    description="Backend API for intelligent code analysis and visualization with AI-powered explanations",
    version="0.2.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# üåê CORS –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –≤—Å–µ—Ö –ø–æ—Ä—Ç–æ–≤ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:3000", "http://127.0.0.1:3000",  # –û—Å–Ω–æ–≤–Ω–æ–π –ø–æ—Ä—Ç Next.js
        "http://localhost:3001", "http://127.0.0.1:3001",  # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ—Ä—Ç
        "http://localhost:3002", "http://127.0.0.1:3002",  # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ—Ä—Ç
        "http://localhost:3003", "http://127.0.0.1:3003",  # –†–µ–∑–µ—Ä–≤–Ω—ã–π –ø–æ—Ä—Ç
        "http://192.168.156.236:3000"  # –í–∞—à –≤–Ω–µ—à–Ω–∏–π –∞–¥—Ä–µ—Å —Ñ—Ä–æ–Ω—Ç–∞
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
def init_database():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è SQLite –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
    conn = sqlite3.connect("code_analyzer.db")
    cursor = conn.cursor()
    
    # –¢–∞–±–ª–∏—Ü–∞ –ø—Ä–æ–µ–∫—Ç–æ–≤
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS projects (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            name TEXT NOT NULL,
            path TEXT NOT NULL UNIQUE,
            language TEXT,
            framework TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    """)
    
    # –¢–∞–±–ª–∏—Ü–∞ –∞–Ω–∞–ª–∏–∑–æ–≤
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS analyses (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            project_id INTEGER,
            analysis_type TEXT NOT NULL,
            results TEXT, -- JSON
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            FOREIGN KEY (project_id) REFERENCES projects (id)
        )
    """)
    
    # –¢–∞–±–ª–∏—Ü–∞ –æ–±—É—á–∞—é—â–∏—Ö —Å–µ—Å—Å–∏–π
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS learning_sessions (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            user_id TEXT,
            topic TEXT,
            progress TEXT, -- JSON
            completed_at TIMESTAMP,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    """)
    
    conn.commit()
    conn.close()

# –£—Ç–∏–ª–∏—Ç—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–¥–∞
class CodeAnalyzer:
    @staticmethod
    def analyze_file(file_path: str) -> FileInfo:
        """–ê–Ω–∞–ª–∏–∑ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞"""
        import re # Ensure re is imported here
        path_obj = Path(file_path)
        
        if not path_obj.exists():
            raise HTTPException(status_code=404, detail="File not found")
        
        file_info = FileInfo(
            path=str(path_obj),
            name=path_obj.name,
            type=path_obj.suffix[1:] if path_obj.suffix else "unknown",
            size=path_obj.stat().st_size,
            lines_of_code=0,
            functions=[],
            imports=[],
            todos=[],
            doc_details=[]
        )
        
        # Regex patterns for TODOs, FIXMEs, HACKs
        # Covers #, //, /* ... */, <!-- ... -->, """ ... """, ''' ... '''
        todo_patterns = [
            re.compile(r"#\s*(TODO|FIXME|HACK)\s*[:\-]\s*(.*)", re.IGNORECASE),
            re.compile(r"//\s*(TODO|FIXME|HACK)\s*[:\-]\s*(.*)", re.IGNORECASE),
            re.compile(r"/\*\s*(TODO|FIXME|HACK)\s*[:\-]\s*(.*?)\s*\*/", re.IGNORECASE | re.DOTALL),
            re.compile(r"<!--\s*(TODO|FIXME|HACK)\s*[:\-]\s*(.*?)\s*-->", re.IGNORECASE | re.DOTALL),
            # Python docstrings (multiline) - basic check, might need refinement for perfect parsing
            re.compile(r"\"\"\"\s*(TODO|FIXME|HACK)\s*[:\-]\s*(.*?)\s*\"\"\"", re.IGNORECASE | re.DOTALL),
            re.compile(r"'''\s*(TODO|FIXME|HACK)\s*[:\-]\s*(.*?)\s*'''", re.IGNORECASE | re.DOTALL),
        ]

        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                content_lines = f.readlines()
                file_info.lines_of_code = len(content_lines)

                full_content = "".join(content_lines)

                # Scan for TODOs/FIXMEs
                # For line-by-line comments (#, //)
                for i, line_text in enumerate(content_lines):
                    for pattern in todo_patterns[:2]: # Only line comment patterns
                        match = pattern.search(line_text)
                        if match:
                            file_info.todos.append({
                                "line": i + 1,
                                "type": match.group(1).upper(),
                                "content": match.group(2).strip(),
                                "priority": None # Placeholder for future enhancement
                            })

                # For block comments (/* ... */, <!-- ... -->, """...""", '''...''')
                # These need to be searched in the full content, line numbers are approximations (start of match)
                for pattern in todo_patterns[2:]:
                    for match in pattern.finditer(full_content):
                        start_char_index = match.start()
                        # Approximate line number
                        line_num = full_content.count('\n', 0, start_char_index) + 1
                        file_info.todos.append({
                            "line": line_num,
                            "type": match.group(1).upper(),
                            "content": match.group(2).strip().replace('\n', ' '), # Flatten multiline content
                            "priority": None
                        })

                # Existing analysis for functions and imports
                if path_obj.suffix in ['.js', '.ts', '.tsx', '.jsx']:
                    functions = re.findall(r'function\s+(\w+)|const\s+(\w+)\s*=.*?=>|(\w+)\s*:\s*\([^)]*\)\s*=>', full_content)
                    file_info.functions = [f for func_group in functions for f in func_group if f]
                    imports = re.findall(r'import.*?from\s+[\'"]([^\'"]+)[\'"]', full_content)
                    file_info.imports = imports

                elif path_obj.suffix == '.py':
                    functions = re.findall(r'def\s+(\w+)', full_content)
                    file_info.functions = functions
                    imports = re.findall(r'from\s+(\S+)\s+import|import\s+(\S+)', full_content)
                    file_info.imports = [imp for imp_group in imports for imp in imp_group if imp]
                    
                    # Python Docstring Parsing using AST
                    try:
                        tree = ast.parse(full_content, filename=file_path)
                        for node in ast.walk(tree):
                            if isinstance(node, ast.FunctionDef):
                                docstring = ast.get_docstring(node)
                                parsed_function = DocFunction(name=node.name, line_start=node.lineno, line_end=node.end_lineno)
                                if docstring:
                                    # Simple parsing for now, can be expanded
                                    lines = [line.strip() for line in docstring.split('\n')]
                                    parsed_function.description = lines[0] if lines else None

                                    param_matches = re.finditer(r":param\s+(?:([\w\s]+)\s*:\s*)?(\w+)\s*:(.*)", docstring) # :param type name: desc or :param name: desc
                                    for match in param_matches:
                                        param_type, param_name, param_desc = match.groups()
                                        parsed_function.params.append(DocFunctionParam(name=param_name.strip(), type=param_type.strip() if param_type else None, description=param_desc.strip()))

                                    return_match = re.search(r":return(?:s)?\s*(?:([\w\s\[\],\|]+)\s*:\s*)?(.*)|:rtype:\s*([\w\s\[\],\|]+)", docstring, re.DOTALL) # :return type: desc or :returns: desc or :rtype: type
                                    if return_match:
                                        g = return_match.groups()
                                        # g[0] is type from :return type:, g[1] is desc from :return ...: desc, g[2] is type from :rtype:
                                        return_type = g[0] or g[2]
                                        return_desc = g[1]
                                        parsed_function.returns = {"type": return_type.strip() if return_type else None, "description": return_desc.strip() if return_desc else None}
                                file_info.doc_details.append(parsed_function)
                    except Exception as e:
                        print(f"Error parsing Python AST for {file_path}: {e}")

                elif path_obj.suffix in ['.js', '.ts', '.tsx', '.jsx']:
                    # JSDoc Parsing (Simplified Regex)
                    # This regex is basic and may need significant improvement for complex cases or various JSDoc styles.
                    # It tries to find a JSDoc block and the function/method name that follows it.
                    jsdoc_func_pattern = re.compile(
                        r"/\*\*(.*?)\*/\s*(?:export\s+)?(?:async\s+)?(?:function\s*(?P<funcName1>\w+)\s*\(|const\s+(?P<funcName2>\w+)\s*=\s*(?:async)?\s*\(|(?P<methodName>\w+)\s*\([^)]*\)\s*\{)",
                        re.DOTALL | re.MULTILINE
                    )
                    for match in jsdoc_func_pattern.finditer(full_content):
                        jsdoc_content = match.group(1)
                        func_name = match.group('funcName1') or match.group('funcName2') or match.group('methodName')
                        if not func_name: continue

                        parsed_function = DocFunction(name=func_name)
                        
                        desc_match = re.search(r"@description\s+([^\n@]+)|([^\n@]+)", jsdoc_content, re.DOTALL) # First non-tag line or @description
                        if desc_match:
                            parsed_function.description = (desc_match.group(1) or desc_match.group(2) or "").strip()

                        for param_match in re.finditer(r"@param\s+\{(.*?)\}\s+(\w+)\s*(?:-\s*(.*?))?\s*(?=\n|\@)", jsdoc_content, re.DOTALL):
                            param_type, param_name, param_desc = param_match.groups()
                            parsed_function.params.append(DocFunctionParam(name=param_name.strip(), type=param_type.strip(), description=(param_desc or "").strip()))
                        
                        returns_match = re.search(r"@returns?\s+\{(.*?)\}\s*(.*)|@returns?\s+(.*)", jsdoc_content, re.DOTALL)
                        if returns_match:
                            g = returns_match.groups()
                            # g[0] is type from {@type}, g[1] is desc from {@type} desc, g[2] is desc from @returns desc
                            return_type = g[0]
                            return_desc = g[1] or g[2]
                            parsed_function.returns = {"type": return_type.strip() if return_type else None, "description": return_desc.strip() if return_desc else None}
                        
                        file_info.doc_details.append(parsed_function)

        except Exception as e:
            print(f"Error analyzing file content for {file_path}: {e}")
        
        return file_info
    
    @staticmethod
    async def analyze_project_monitored(
        project_path: str, 
        session_id: str,
        include_tests: bool = True,
        analysis_depth: str = "medium"
    ) -> ProjectAnalysisResult:
        """
        üöÄ –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø—Ä–æ–µ–∫—Ç–∞ —Å –ø–æ–ª–Ω—ã–º –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º
        –£—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –∞–Ω–∞–ª–∏–∑–∞ —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ–º –∫–∞–∂–¥–æ–≥–æ —ç—Ç–∞–ø–∞
        """
        from concurrent.futures import ThreadPoolExecutor, as_completed
        
        async with analytics_logger.track_operation(
            EventType.FILE_SCAN_START,
            project_path=project_path,
            session_id=session_id
        ):
            # üìÅ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—é —Ñ–∞–π–ª–æ–≤
            path_obj = Path(project_path)
            
            if not path_obj.exists():
                raise HTTPException(status_code=404, detail="Project path not found")
            
            files = []
            dependencies = []
            file_paths = []
            
            # üîç –°–∫–∞–Ω–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã –ø—Ä–æ–µ–∫—Ç–∞ —Å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º
            logger.info(f"üîç –ù–∞—á–∏–Ω–∞–µ–º —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤ –≤ –ø—Ä–æ–µ–∫—Ç–µ: {project_path}")
            
            for file_path in path_obj.rglob("*"):
                if file_path.is_file() and file_path.suffix in ['.js', '.ts', '.tsx', '.jsx', '.py', '.html', '.css']:
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ –ø–∞–ø–∫–∏
                    if any(part in str(file_path) for part in ['node_modules', '.git', 'dist', 'build', '__pycache__']):
                        continue
                    file_paths.append(str(file_path))
            
            # üìä –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            max_files = 1000 if analysis_depth == "deep" else 500 if analysis_depth == "medium" else 200
            if len(file_paths) > max_files:
                logger.info(f"üìä –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∞–Ω–∞–ª–∏–∑ –¥–æ {max_files} —Ñ–∞–π–ª–æ–≤ –∏–∑ {len(file_paths)} –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö")
                file_paths = file_paths[:max_files]
            
            logger.info(f"üìÅ –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: {len(file_paths)}")
        
        # üîÑ –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–æ–≤ —Å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º
        async with analytics_logger.track_operation(
            EventType.FILE_ANALYSIS_START,
            project_path=project_path,
            metadata={"files_count": len(file_paths), "analysis_depth": analysis_depth},
            session_id=session_id
        ):
            
            total_files = len(file_paths)
            processed_files = 0
            
            logger.info(f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ {total_files} —Ñ–∞–π–ª–æ–≤...")
            
            def analyze_single_file_monitored(file_path_str: str) -> Optional[FileInfo]:
                """–ê–Ω–∞–ª–∏–∑ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ —Å –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–º –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º"""
                try:
                    # –î–ª—è –∫–∞–∂–¥–æ–≥–æ —Ñ–∞–π–ª–∞ —Å–æ–∑–¥–∞—ë–º –æ—Ç–¥–µ–ª—å–Ω–æ–µ —Å–æ–±—ã—Ç–∏–µ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
                    start_time = time.time()
                    
                    result = CodeAnalyzer.analyze_file(file_path_str)
                    
                    duration_ms = (time.time() - start_time) * 1000
                    
                    # –õ–æ–≥–∏—Ä—É–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∞–π–ª–∞
                    log_analysis_event(
                        EventType.FILE_ANALYSIS_COMPLETE,
                        project_path=project_path,
                        file_path=file_path_str,
                        duration_ms=duration_ms,
                        metadata={
                            "file_size": result.size,
                            "lines_of_code": result.lines_of_code,
                            "functions_count": len(result.functions)
                        },
                        session_id=session_id
                    )
                    
                    return result
                    
                except Exception as e:
                    # –õ–æ–≥–∏—Ä—É–µ–º –æ—à–∏–±–∫—É –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∞–π–ª–∞
                    log_analysis_event(
                        EventType.ANALYSIS_ERROR,
                        project_path=project_path,
                        file_path=file_path_str,
                        error_message=str(e),
                        metadata={"error_type": "file_analysis_error"},
                        session_id=session_id
                    )
                    logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∞–π–ª–∞ {file_path_str}: {str(e)}")
                    return None
            
            # üîÑ –í—ã–ø–æ–ª–Ω—è–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
            with ThreadPoolExecutor(max_workers=8) as executor:
                future_to_path = {
                    executor.submit(analyze_single_file_monitored, path): path 
                    for path in file_paths
                }
                
                for future in as_completed(future_to_path):
                    path = future_to_path[future]
                    try:
                        file_info = future.result()
                        if file_info:
                            files.append(file_info)
                        processed_files += 1
                        
                        # üìä –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 10 —Ñ–∞–π–ª–æ–≤
                        if processed_files % 10 == 0 or processed_files == total_files:
                            progress_percentage = (processed_files / total_files) * 100
                            logger.info(f"üìà –ü—Ä–æ–≥—Ä–µ—Å—Å: {processed_files}/{total_files} —Ñ–∞–π–ª–æ–≤ ({progress_percentage:.1f}%)")
                            
                            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å–æ–±—ã—Ç–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
                            log_analysis_event(
                                EventType.PERFORMANCE_METRIC,
                                project_path=project_path,
                                metadata={
                                    "progress_percentage": progress_percentage,
                                    "files_processed": processed_files,
                                    "files_total": total_files
                                },
                                session_id=session_id
                            )
                            
                    except Exception as e:
                        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±—É–¥—É—â–µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –¥–ª—è {path}: {e}")
            
            logger.info(f"‚úÖ –ê–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–æ–≤ –∑–∞–≤–µ—Ä—à—ë–Ω. –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(files)} –∏–∑ {total_files}")
        
        # üîó –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π —Å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º
        logger.info("üîó –°—Ç—Ä–æ–∏–º –≥—Ä–∞—Ñ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π...")
        
        all_project_todos = []
        project_docs_list: List[DocFile] = []

        for file_info in files:
            # Dependencies
            for import_path in file_info.imports:
                dependencies.append({
                    "from": file_info.path,
                    "to": import_path,
                    "type": "import"
                })
            # Aggregate TODOs
            if file_info.todos:
                for todo in file_info.todos:
                    all_project_todos.append({
                        "file_path": file_info.path,
                        "line": todo["line"],
                        "type": todo["type"],
                        "content": todo["content"],
                        "priority": todo.get("priority")
                    })
            # Aggregate Documentation
            if file_info.doc_details and len(file_info.doc_details) > 0:
                project_docs_list.append(DocFile(
                    file_path=file_info.path,
                    functions=file_info.doc_details
                ))
        
        # üìä –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ —Å –ø–æ–¥—Ä–æ–±–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–æ–π
        total_lines = sum(f.lines_of_code or 0 for f in files)
        total_functions = sum(len(f.functions) for f in files)
        
        metrics = {
            "total_files": len(files),
            "total_lines": total_lines,
            "total_functions": total_functions,
            "avg_lines_per_file": total_lines / len(files) if files else 0,
            "languages": list(set(f.type for f in files if f.type != "unknown")),
            "analysis_depth": analysis_depth,
            "analysis_timestamp": datetime.now(timezone.utc).isoformat()
        }
        
        # üèóÔ∏è –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Å –ò–ò-–ø–æ–º–æ—â—å—é
        patterns = []
        if any("component" in f.path.lower() for f in files):
            patterns.append("Component Architecture")
        if any("api" in f.path.lower() or "service" in f.path.lower() for f in files):
            patterns.append("Service Layer")
        if any("test" in f.path.lower() for f in files):
            patterns.append("Test Coverage")
        
        # üìà –§–∏–Ω–∞–ª—å–Ω–∞—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∞
        final_result = ProjectAnalysisResult(
            project_path=project_path,
            files=files,
            dependencies=dependencies,
            metrics=metrics,
            architecture_patterns=patterns,
            all_todos=all_project_todos,
            project_documentation=project_docs_list
        )
        
        # üéØ –õ–æ–≥–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        log_analysis_event(
            EventType.ANALYSIS_COMPLETE,
            project_path=project_path,
            metadata={
                "final_metrics": metrics,
                "patterns_detected": patterns,
                "total_todos": len(all_project_todos),
                "documentation_files": len(project_docs_list)
            },
            session_id=session_id
        )
        
        logger.info(f"üéâ –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø—Ä–æ–µ–∫—Ç–∞ –∑–∞–≤–µ—Ä—à—ë–Ω —É—Å–ø–µ—à–Ω–æ!")
        logger.info(f"üìä –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        logger.info(f"   üìÅ –§–∞–π–ª–æ–≤: {metrics['total_files']}")
        logger.info(f"   üìù –°—Ç—Ä–æ–∫ –∫–æ–¥–∞: {metrics['total_lines']:,}")
        logger.info(f"   ‚öôÔ∏è –§—É–Ω–∫—Ü–∏–π: {metrics['total_functions']}")
        logger.info(f"   üîó –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π: {len(dependencies)}")
        logger.info(f"   üìã TODO —ç–ª–µ–º–µ–Ω—Ç–æ–≤: {len(all_project_todos)}")
        logger.info(f"   üèóÔ∏è –ü–∞—Ç—Ç–µ—Ä–Ω–æ–≤: {len(patterns)}")
        
        return final_result
        """–ê–Ω–∞–ª–∏–∑ –≤—Å–µ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —Å–∫–æ—Ä–æ—Å—Ç–∏"""
        from concurrent.futures import ThreadPoolExecutor, as_completed
        path_obj = Path(project_path)
        
        if not path_obj.exists():
            raise HTTPException(status_code=404, detail="Project path not found")
        
        files = []
        dependencies = []
        file_paths = []
        
        # –°–∫–∞–Ω–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã –ø—Ä–æ–µ–∫—Ç–∞ –∏ —Å–æ–±–∏—Ä–∞–µ–º —Å–ø–∏—Å–æ–∫ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        for file_path in path_obj.rglob("*"):
            if file_path.is_file() and file_path.suffix in ['.js', '.ts', '.tsx', '.jsx', '.py', '.html', '.css']:
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º node_modules –∏ –¥—Ä—É–≥–∏–µ —Å–ª—É–∂–µ–±–Ω—ã–µ –ø–∞–ø–∫–∏
                if any(part in str(file_path) for part in ['node_modules', '.git', 'dist', 'build', '__pycache__']):
                    continue
                file_paths.append(str(file_path))
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ø–µ—Ä–µ–≥—Ä—É–∑–∫–∏
        max_files = 500
        if len(file_paths) > max_files:
            print(f"Project has {len(file_paths)} files, limiting analysis to first {max_files} files for performance.")
            file_paths = file_paths[:max_files]
        
        # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–æ–≤ —Å –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–º –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        total_files = len(file_paths)
        processed_files = 0
        print(f"Starting analysis of {total_files} files...")
        with ThreadPoolExecutor(max_workers=10) as executor:
            future_to_path = {executor.submit(CodeAnalyzer.analyze_file, path): path for path in file_paths}
            for future in as_completed(future_to_path):
                path = future_to_path[future]
                try:
                    file_info = future.result()
                    files.append(file_info)
                    processed_files += 1
                    print(f"Processed {processed_files}/{total_files} files ({(processed_files/total_files)*100:.1f}%) - {path}")
                except Exception as e:
                    print(f"Error analyzing {path}: {e}")
        print("Analysis complete.")
        
        # –°—Ç—Ä–æ–∏–º –≥—Ä–∞—Ñ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π, —Å–æ–±–∏—Ä–∞–µ–º –≤—Å–µ TODOs –∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é
        all_project_todos = []
        project_docs_list: List[DocFile] = []

        for file_info in files:
            # Dependencies
            for import_path in file_info.imports:
                dependencies.append({
                    "from": file_info.path,
                    "to": import_path,
                    "type": "import"
                })
            # Aggregate TODOs
            if file_info.todos:
                for todo in file_info.todos:
                    all_project_todos.append({
                        "file_path": file_info.path,
                        "line": todo["line"],
                        "type": todo["type"],
                        "content": todo["content"],
                        "priority": todo.get("priority")
                    })
            # Aggregate Documentation
            if file_info.doc_details and len(file_info.doc_details) > 0:
                project_docs_list.append(DocFile(
                    file_path=file_info.path,
                    functions=file_info.doc_details
                ))
        
        # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
        total_lines = sum(f.lines_of_code or 0 for f in files)
        total_functions = sum(len(f.functions) for f in files)
        
        metrics = {
            "total_files": len(files),
            "total_lines": total_lines,
            "total_functions": total_functions,
            "avg_lines_per_file": total_lines / len(files) if files else 0,
            "languages": list(set(f.type for f in files if f.type != "unknown"))
        }
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã (—É–ø—Ä–æ—â–µ–Ω–Ω–æ)
        patterns = []
        if any("component" in f.path.lower() for f in files):
            patterns.append("Component Architecture")
        if any("api" in f.path.lower() or "service" in f.path.lower() for f in files):
            patterns.append("Service Layer")
        if any("test" in f.path.lower() for f in files):
            patterns.append("Test Coverage")
        
        return ProjectAnalysisResult(
            project_path=project_path,
            files=files,
            dependencies=dependencies,
            metrics=metrics,
            architecture_patterns=patterns,
            all_todos=all_project_todos,
            project_documentation=project_docs_list
        )

# API Endpoints
@app.get("/")
async def root():
    """–ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ API"""
    return {
        "message": "MCP Code Analyzer API",
        "version": "0.1.0",
        "status": "running",
        "endpoints": {
            "docs": "/docs",
            "analyze": "/api/analyze",
            "explain": "/api/explain",
            "projects": "/api/projects"
        }
    }

@app.post("/api/analyze", response_model=ProjectAnalysisResult)
async def analyze_project(request: ProjectAnalysisRequest):
    """
    üöÄ –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø—Ä–æ–µ–∫—Ç–∞ —Å –ø–æ–ª–Ω—ã–º –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º
    –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–¥–∞ —Å AI –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π –∏ –¥–µ—Ç–∞–ª—å–Ω—ã–º –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ–º
    """
    
    # üÜî –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID —Å–µ—Å—Å–∏–∏ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è
    session_id = str(uuid.uuid4())
    
    try:
        # üéØ –ù–∞—á–∏–Ω–∞–µ–º –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –≤—Å–µ–π –æ–ø–µ—Ä–∞—Ü–∏–∏ –∞–Ω–∞–ª–∏–∑–∞
        async with analytics_logger.track_operation(
            EventType.ANALYSIS_START,
            project_path=request.path,
            metadata={
                "include_tests": request.include_tests,
                "analysis_depth": request.analysis_depth,
                "request_timestamp": datetime.now().isoformat()
            },
            session_id=session_id
        ) as operation_id:
            
            # üìä –õ–æ–≥–∏—Ä—É–µ–º –Ω–∞—á–∞–ª–æ –∞–Ω–∞–ª–∏–∑–∞
            logger.info(f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø—Ä–æ–µ–∫—Ç–∞: {request.path}")
            logger.info(f"üìã –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∞–Ω–∞–ª–∏–∑–∞: –≥–ª—É–±–∏–Ω–∞={request.analysis_depth}, —Ç–µ—Å—Ç—ã={request.include_tests}")
            logger.info(f"üÜî ID —Å–µ—Å—Å–∏–∏: {session_id}")
            
            # üîç –ó–∞–ø—É—Å–∫–∞–µ–º –∞–Ω–∞–ª–∏–∑ —Å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º
            result = await CodeAnalyzer.analyze_project_monitored(
                request.path, 
                session_id,
                include_tests=request.include_tests,
                analysis_depth=request.analysis_depth
            )
            
            # üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –±–∞–∑—É —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
            conn = sqlite3.connect("code_analyzer.db")
            cursor = conn.cursor()
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–µ–∫—Ç
            cursor.execute("""
                INSERT OR REPLACE INTO projects (name, path, language)
                VALUES (?, ?, ?)
            """, (
                os.path.basename(request.path),
                request.path,
                result.metrics.get("languages", ["unknown"])[0] if result.metrics.get("languages") else "unknown"
            ))
            
            project_id = cursor.lastrowid
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞–Ω–∞–ª–∏–∑ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ —Å–µ—Å—Å–∏–∏
            analysis_metadata = {
                "session_id": session_id,
                "operation_id": operation_id,
                "analysis_depth": request.analysis_depth,
                "include_tests": request.include_tests,
                "completion_time": datetime.now().isoformat()
            }
            
            cursor.execute("""
                INSERT INTO analyses (project_id, analysis_type, results)
                VALUES (?, ?, ?)
            """, (
                project_id, 
                "full_analysis_monitored", 
                json.dumps({
                    **result.dict(),
                    "analysis_metadata": analysis_metadata
                })
            ))
            
            conn.commit()
            conn.close()
            
            # üéâ –õ–æ–≥–∏—Ä—É–µ–º —É—Å–ø–µ—à–Ω–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ
            analytics_summary = get_analytics_summary(session_id)
            logger.info(f"‚úÖ –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–µ–∫—Ç–∞ –∑–∞–≤–µ—Ä—à—ë–Ω —É—Å–ø–µ—à–Ω–æ!")
            logger.info(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã: {len(result.files)} —Ñ–∞–π–ª–æ–≤, {result.metrics['total_lines']} —Å—Ç—Ä–æ–∫, {result.metrics['total_functions']} —Ñ—É–Ω–∫—Ü–∏–π")
            logger.info(f"üìà –ê–Ω–∞–ª–∏—Ç–∏–∫–∞ —Å–µ—Å—Å–∏–∏: {analytics_summary}")
            
            return result
            
    except Exception as e:
        # üö® –î–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—à–∏–±–∫–∏
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –ø—Ä–æ–µ–∫—Ç–∞ {request.path}: {str(e)}")
        
        # –õ–æ–≥–∏—Ä—É–µ–º –≤ —Å–∏—Å—Ç–µ–º—É –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
        log_analysis_event(
            EventType.ANALYSIS_ERROR,
            project_path=request.path,
            error_message=str(e),
            error_traceback=traceback.format_exc(),
            metadata={
                "request_params": {
                    "include_tests": request.include_tests,
                    "analysis_depth": request.analysis_depth
                },
                "session_id": session_id
            }
        )
        
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/explain", response_model=CodeExplanation)
async def explain_code(request: CodeExplanationRequest):
    """
    –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –∫–æ–¥–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º AI.
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç OpenAI GPT –∏ Anthropic Claude –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ–±—ä—è—Å–Ω–µ–Ω–∏–π.
    """
    global ai_manager
    
    try:
        # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∫–æ–¥–∞ –¥–ª—è AI –∞–Ω–∞–ª–∏–∑–∞
        context = CodeContext(
            file_path=request.file_path or "unknown",
            file_content=request.code,
            file_type=request.language,
            project_info=request.project_context or {},
            dependencies=[],
            functions=[],
            imports=[],
            architecture_patterns=[],
            lines_of_code=len(request.code.split('\n'))
        )
        
        # –ï—Å–ª–∏ AI –º–µ–Ω–µ–¥–∂–µ—Ä –¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
        if ai_manager and ai_manager.services:
            try:
                # –ü–æ–ª—É—á–∞–µ–º —É–º–Ω–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –æ—Ç AI
                ai_response = await ai_manager.explain_code_smart(context, request.level)
                
                if ai_response:
                    # –¢–∞–∫–∂–µ –ø–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ —É–ª—É—á—à–µ–Ω–∏—é –∏ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
                    improvements_task = ai_manager.suggest_improvements_smart(context)
                    patterns_task = ai_manager.detect_patterns_smart(context)
                    
                    improvements, patterns = await asyncio.gather(
                        improvements_task, patterns_task, return_exceptions=True
                    )
                    
                    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (–∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏)
                    improvements = improvements if not isinstance(improvements, Exception) else []
                    patterns = patterns if not isinstance(patterns, Exception) else []
                    
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä
                    used_provider = "unknown"
                    for provider, service in ai_manager.services.items():
                        if service.request_count > 0:
                            used_provider = provider.value
                            break
                    
                    return CodeExplanation(
                        explanation=ai_response.explanation,
                        concepts=ai_response.concepts,
                        examples=ai_response.examples,
                        recommendations=ai_response.recommendations,
                        improvements=improvements[:5],  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
                        patterns=patterns[:5],
                        confidence_score=ai_response.confidence_score,
                        ai_provider=used_provider
                    )
                    
            except AIServiceError as e:
                logger.warning(f"AI —Å–µ—Ä–≤–∏—Å –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {str(e)}. –ò—Å–ø–æ–ª—å–∑—É–µ–º fallback.")
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ AI –∞–Ω–∞–ª–∏–∑–∞: {str(e)}. –ò—Å–ø–æ–ª—å–∑—É–µ–º fallback.")
        
        # Fallback: –ø—Ä–æ—Å—Ç–æ–π –∞–Ω–∞–ª–∏–∑ –±–µ–∑ AI
        logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è fallback –∞–Ω–∞–ª–∏–∑ –±–µ–∑ AI")
        explanation = f"–≠—Ç–æ—Ç {request.language} –∫–æ–¥ –≤—ã–ø–æ–ª–Ω—è–µ—Ç —Å–ª–µ–¥—É—é—â–∏–µ –æ–ø–µ—Ä–∞—Ü–∏–∏..."
        
        # –ü—Ä–æ—Å—Ç–æ–π –∞–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ü–µ–ø—Ü–∏–π
        concepts = []
        if "function" in request.code:
            concepts.append("functions")
        if any(keyword in request.code for keyword in ["const", "let", "var"]):
            concepts.append("variables")
        if "import" in request.code:
            concepts.append("modules")
        if "class" in request.code:
            concepts.append("classes")
        if "async" in request.code or "await" in request.code:
            concepts.append("asynchronous programming")
        
        return CodeExplanation(
            explanation=explanation,
            concepts=concepts,
            examples=["–ü—Ä–∏–º–µ—Ä 1: –±–∞–∑–æ–≤–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ", "–ü—Ä–∏–º–µ—Ä 2: —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ª—É—á–∞–π"],
            recommendations=["–î–æ–±–∞–≤—å—Ç–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏", "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –æ–ø–∏—Å–∞—Ç–µ–ª—å–Ω—ã–µ –∏–º–µ–Ω–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö"],
            improvements=[],
            patterns=[],
            confidence_score=0.5,
            ai_provider="fallback"
        )
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—ä—è—Å–Ω–µ–Ω–∏–∏ –∫–æ–¥–∞: {str(e)}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –∫–æ–¥–∞: {str(e)}")

@app.post("/api/comprehensive-analysis", response_model=ComprehensiveAnalysisResult)
async def comprehensive_analysis(request: ComprehensiveAnalysisRequest):
    """
    –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π AI-–∞–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö AI —Å–µ—Ä–≤–∏—Å–æ–≤.
    –ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ, —É–ª—É—á—à–µ–Ω–∏—è –∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤.
    """
    global ai_manager
    
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞
        if not Path(request.file_path).exists():
            raise HTTPException(status_code=404, detail="–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ñ–∞–π–ª
        file_info = CodeAnalyzer.analyze_file(request.file_path)
        
        # –ß–∏—Ç–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞
        with open(request.file_path, 'r', encoding='utf-8') as f:
            file_content = f.read()
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–æ–µ–∫—Ç –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        try:
            project_analysis = CodeAnalyzer.analyze_project(request.project_path)
            project_context = {
                "total_files": project_analysis.metrics["total_files"],
                "total_lines": project_analysis.metrics["total_lines"],
                "languages": project_analysis.metrics["languages"],
                "architecture_patterns": project_analysis.architecture_patterns
            }
        except Exception:
            project_context = {}
        
        # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è AI
        context = CodeContext(
            file_path=request.file_path,
            file_content=file_content,
            file_type=file_info.type,
            project_info=project_context,
            dependencies=[],
            functions=file_info.functions,
            imports=file_info.imports,
            architecture_patterns=project_context.get("architecture_patterns", []),
            lines_of_code=file_info.lines_of_code or 0
        )
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
        if ai_manager and ai_manager.services:
            try:
                results = await ai_manager.comprehensive_analysis(context, request.explanation_level)
                
                # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                explanation_data = None
                if results.get("explanation"):
                    explanation_data = {
                        "text": results["explanation"].explanation,
                        "concepts": results["explanation"].concepts,
                        "recommendations": results["explanation"].recommendations,
                        "confidence": results["explanation"].confidence_score
                    }
                
                return ComprehensiveAnalysisResult(
                    explanation=explanation_data,
                    improvements=results.get("improvements", []),
                    patterns=results.get("patterns", []),
                    analysis_metadata=results.get("analysis_metadata", {})
                )
                
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ AI –∞–Ω–∞–ª–∏–∑–∞: {str(e)}")
        
        # Fallback –∞–Ω–∞–ª–∏–∑
        return ComprehensiveAnalysisResult(
            explanation={
                "text": f"–§–∞–π–ª {Path(request.file_path).name} —Å–æ–¥–µ—Ä–∂–∏—Ç {len(file_info.functions)} —Ñ—É–Ω–∫—Ü–∏–π –∏ {file_info.lines_of_code} —Å—Ç—Ä–æ–∫ –∫–æ–¥–∞.",
                "concepts": ["file analysis", "code structure"],
                "recommendations": ["AI –∞–Ω–∞–ª–∏–∑ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"],
                "confidence": 0.3
            },
            improvements=["AI —Å–µ—Ä–≤–∏—Å—ã –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"],
            patterns=["–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ"],
            analysis_metadata={
                "timestamp": "fallback",
                "ai_available": False
            }
        )
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/ai-status")
async def get_ai_status():
    """
    –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ AI —Å–µ—Ä–≤–∏—Å–æ–≤ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è.
    """
    global ai_manager
    
    if not ai_manager:
        return {
            "status": "not_initialized",
            "available_services": [],
            "usage_stats": {}
        }
    
    available_services = []
    for provider in ai_manager.services.keys():
        available_services.append(provider.value)
    
    usage_stats = ai_manager.get_all_usage_stats()
    
    return {
        "status": "initialized" if available_services else "no_services",
        "available_services": available_services,
        "usage_stats": usage_stats,
        "total_requests": sum(stats.get("request_count", 0) for stats in usage_stats.values()),
        "total_tokens": sum(stats.get("total_tokens_used", 0) for stats in usage_stats.values())
    }

@app.get("/api/projects")
async def get_projects():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –ø—Ä–æ–µ–∫—Ç–æ–≤"""
    conn = sqlite3.connect("code_analyzer.db")
    cursor = conn.cursor()
    
    cursor.execute("""
        SELECT id, name, path, language, created_at, updated_at
        FROM projects
        ORDER BY updated_at DESC
    """)
    
    projects = []
    for row in cursor.fetchall():
        projects.append({
            "id": row[0],
            "name": row[1],
            "path": row[2],
            "language": row[3],
            "created_at": row[4],
            "updated_at": row[5]
        })
    
    conn.close()
    return {"projects": projects}

@app.get("/api/health")
async def health_check():
    """
    ü©∫ –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–∏—Å—Ç–µ–º—ã
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
    """
    try:
        # üîç –ü–æ–ª—É—á–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–¥–æ—Ä–æ–≤—å–µ —Å–∏—Å—Ç–µ–º—ã
        health_data = get_system_health()
        
        # üß† –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å AI —Å–µ—Ä–≤–∏—Å–æ–≤
        ai_status = "unknown"
        ai_services = []
        
        global ai_manager
        if ai_manager and ai_manager.services:
            ai_status = "healthy"
            for provider, service in ai_manager.services.items():
                ai_services.append({
                    "provider": provider.value,
                    "status": "active",
                    "request_count": getattr(service, 'request_count', 0)
                })
        else:
            ai_status = "unavailable"
        
        # üíæ –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
        db_status = "unknown"
        try:
            conn = sqlite3.connect("code_analyzer.db")
            cursor = conn.cursor()
            cursor.execute("SELECT COUNT(*) FROM projects")
            projects_count = cursor.fetchone()[0]
            cursor.execute("SELECT COUNT(*) FROM analyses")
            analyses_count = cursor.fetchone()[0]
            conn.close()
            db_status = "healthy"
        except Exception as e:
            db_status = f"error: {str(e)}"
            projects_count = 0
            analyses_count = 0
        
        # üìä –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –æ—Ç—á—ë—Ç –æ –∑–¥–æ—Ä–æ–≤—å–µ
        comprehensive_health = {
            **health_data,
            "services": {
                "ai_engine": {
                    "status": ai_status,
                    "services": ai_services
                },
                "database": {
                    "status": db_status,
                    "projects_count": projects_count,
                    "analyses_count": analyses_count
                },
                "monitoring": {
                    "status": "active",
                    "events_logged": len(analytics_logger.events),
                    "active_sessions": len(analytics_logger.session_stats)
                }
            },
            "api_info": {
                "version": "0.2.0",
                "uptime_events": len(analytics_logger.events),
                "last_restart": datetime.now(timezone.utc).isoformat()
            }
        }
        
        return comprehensive_health
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –∑–¥–æ—Ä–æ–≤—å—è —Å–∏—Å—Ç–µ–º—ã: {str(e)}")
        return {
            "status": "error",
            "error": str(e),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }

@app.get("/api/analytics")
async def get_system_analytics(session_id: Optional[str] = None):
    """
    üìä –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ —Å–∏—Å—Ç–µ–º—ã
    –ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    """
    try:
        # üéØ –ü–æ–ª—É—á–∞–µ–º –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫—É—é —Å–≤–æ–¥–∫—É
        analytics_data = get_analytics_summary(session_id)
        
        # üìà –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        recent_events = analytics_logger.events[-50:] if len(analytics_logger.events) > 50 else analytics_logger.events
        
        # üî• –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å–æ–±—ã—Ç–∏—è
        performance_events = [e for e in recent_events if e.performance_metrics]
        
        performance_trends = {
            "avg_cpu_usage": 0,
            "avg_memory_usage": 0,
            "peak_cpu_usage": 0,
            "peak_memory_usage": 0,
            "events_analyzed": len(performance_events)
        }
        
        if performance_events:
            cpu_values = [e.performance_metrics.cpu_usage for e in performance_events]
            memory_values = [e.performance_metrics.memory_usage for e in performance_events]
            
            performance_trends.update({
                "avg_cpu_usage": sum(cpu_values) / len(cpu_values),
                "avg_memory_usage": sum(memory_values) / len(memory_values),
                "peak_cpu_usage": max(cpu_values),
                "peak_memory_usage": max(memory_values)
            })
        
        # üìã –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–∏–ø–∞–º —Å–æ–±—ã—Ç–∏–π
        event_types_stats = {}
        for event in recent_events:
            event_type = event.event_type.value
            if event_type not in event_types_stats:
                event_types_stats[event_type] = 0
            event_types_stats[event_type] += 1
        
        # üéØ –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π –æ—Ç—á—ë—Ç –∞–Ω–∞–ª–∏—Ç–∏–∫–∏
        comprehensive_analytics = {
            **analytics_data,
            "performance_trends": performance_trends,
            "event_types_distribution": event_types_stats,
            "recent_events_count": len(recent_events),
            "system_insights": {
                "most_common_event": max(event_types_stats.items(), key=lambda x: x[1])[0] if event_types_stats else "none",
                "analysis_success_rate": (
                    (analytics_data.get("total_events", 0) - (analytics_data.get("total_events", 0) * analytics_data.get("error_rate", 0) / 100)) / 
                    max(analytics_data.get("total_events", 1), 1) * 100
                ) if analytics_data.get("total_events", 0) > 0 else 100
            },
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
        
        return comprehensive_analytics
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –∞–Ω–∞–ª–∏—Ç–∏–∫–∏: {str(e)}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∏: {str(e)}")

@app.get("/api/monitoring/events")
async def get_recent_events(limit: int = 50, event_type: Optional[str] = None):
    """
    üìã –ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å–æ–±—ã—Ç–∏–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
    –ü–æ–∑–≤–æ–ª—è–µ—Ç –ø—Ä–æ—Å–º–∞—Ç—Ä–∏–≤–∞—Ç—å –¥–µ—Ç–∞–ª—å–Ω—ã–µ –ª–æ–≥–∏ —Å–∏—Å—Ç–µ–º—ã –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
    """
    try:
        # üîç –§–∏–ª—å—Ç—Ä—É–µ–º —Å–æ–±—ã—Ç–∏—è –ø–æ —Ç–∏–ø—É (–µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω)
        events = analytics_logger.events
        
        if event_type:
            try:
                target_event_type = EventType(event_type)
                events = [e for e in events if e.event_type == target_event_type]
            except ValueError:
                raise HTTPException(status_code=400, detail=f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø —Å–æ–±—ã—Ç–∏—è: {event_type}")
        
        # üìù –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å–æ–±—ã—Ç–∏—è
        recent_events = events[-limit:] if len(events) > limit else events
        
        # üìä –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        formatted_events = []
        for event in recent_events:
            formatted_event = {
                "event_id": event.event_id,
                "event_type": event.event_type.value,
                "timestamp": event.timestamp.isoformat(),
                "project_path": event.project_path,
                "file_path": event.file_path,
                "duration_ms": event.duration_ms,
                "error_message": event.error_message,
                "metadata": event.metadata or {},
                "performance_metrics": event.performance_metrics.__dict__ if event.performance_metrics else None
            }
            formatted_events.append(formatted_event)
        
        return {
            "events": formatted_events,
            "total_events_available": len(events),
            "events_returned": len(formatted_events),
            "filter_applied": event_type,
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å–æ–±—ã—Ç–∏–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞: {str(e)}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–æ–±—ã—Ç–∏–π: {str(e)}")

@app.get("/api/monitoring/sessions/{session_id}")
async def get_session_details(session_id: str):
    """
    üîç –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Å–µ—Å—Å–∏–∏ –∞–Ω–∞–ª–∏–∑–∞
    –ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –ø–æ–ª–Ω—É—é –∏—Å—Ç–æ—Ä–∏—é –∏ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
    """
    try:
        # üéØ –ü–æ–ª—É—á–∞–µ–º —Å–æ–±—ã—Ç–∏—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Å–µ—Å—Å–∏–∏
        session_events = [e for e in analytics_logger.events if e.user_session_id == session_id]
        
        if not session_events:
            raise HTTPException(status_code=404, detail=f"–°–µ—Å—Å–∏—è {session_id} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        
        # üìä –í—ã—á–∏—Å–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Å–µ—Å—Å–∏–∏
        session_stats = analytics_logger.session_stats.get(session_id, {})
        
        # ‚è±Ô∏è –ê–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
        start_time = min(e.timestamp for e in session_events)
        end_time = max(e.timestamp for e in session_events)
        total_duration = (end_time - start_time).total_seconds() * 1000  # –≤ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥–∞—Ö
        
        # üìà –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–µ—Å—Å–∏–∏
        performance_data = []
        for event in session_events:
            if event.performance_metrics:
                performance_data.append({
                    "timestamp": event.timestamp.isoformat(),
                    "cpu_usage": event.performance_metrics.cpu_usage,
                    "memory_usage": event.performance_metrics.memory_usage,
                    "event_type": event.event_type.value
                })
        
        # üéØ –§–æ—Ä–º–∏—Ä—É–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á—ë—Ç –ø–æ —Å–µ—Å—Å–∏–∏
        session_details = {
            "session_id": session_id,
            "session_stats": session_stats,
            "timeline": {
                "start_time": start_time.isoformat(),
                "end_time": end_time.isoformat(),
                "total_duration_ms": total_duration,
                "events_count": len(session_events)
            },
            "events_timeline": [
                {
                    "event_id": e.event_id,
                    "event_type": e.event_type.value,
                    "timestamp": e.timestamp.isoformat(),
                    "duration_ms": e.duration_ms,
                    "file_path": e.file_path,
                    "error_message": e.error_message,
                    "metadata": e.metadata or {}
                }
                for e in session_events
            ],
            "performance_timeline": performance_data,
            "error_analysis": {
                "total_errors": len([e for e in session_events if e.error_message]),
                "error_types": list(set(e.error_message for e in session_events if e.error_message))
            },
            "generated_at": datetime.now(timezone.utc).isoformat()
        }
        
        return session_details
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –¥–µ—Ç–∞–ª–µ–π —Å–µ—Å—Å–∏–∏ {session_id}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–µ—Ç–∞–ª–µ–π —Å–µ—Å—Å–∏–∏: {str(e)}")

@app.delete("/api/monitoring/events")
async def clear_monitoring_logs():
    """
    üßπ –û—á–∏—Å—Ç–∫–∞ –ª–æ–≥–æ–≤ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
    –ê–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
    """
    try:
        events_count = len(analytics_logger.events)
        sessions_count = len(analytics_logger.session_stats)
        
        # üóëÔ∏è –û—á–∏—â–∞–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ
        analytics_logger.events.clear()
        analytics_logger.session_stats.clear()
        analytics_logger.active_operations.clear()
        
        logger.info(f"üßπ –õ–æ–≥–∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –æ—á–∏—â–µ–Ω—ã: {events_count} —Å–æ–±—ã—Ç–∏–π, {sessions_count} —Å–µ—Å—Å–∏–π")
        
        return {
            "message": "–õ–æ–≥–∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —É—Å–ø–µ—à–Ω–æ –æ—á–∏—â–µ–Ω—ã",
            "cleared_events": events_count,
            "cleared_sessions": sessions_count,
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—á–∏—Å—Ç–∫–µ –ª–æ–≥–æ–≤ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞: {str(e)}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –ª–æ–≥–æ–≤: {str(e)}")

@app.get("/api/health")
async def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è API"""
    return {"status": "healthy", "timestamp": "2024-01-01T00:00:00Z"}

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ
@app.on_event("startup")
async def startup_event():
    global ai_manager
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
    init_database()
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è AI —Å–µ—Ä–≤–∏—Å–æ–≤
    try:
        ai_manager = initialize_ai_services()
        if ai_manager.services:
            logger.info(f"ü§ñ AI —Å–µ—Ä–≤–∏—Å—ã –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã: {list(ai_manager.services.keys())}")
        else:
            logger.warning("‚ö†Ô∏è AI —Å–µ—Ä–≤–∏—Å—ã –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ OPENAI_API_KEY –∏–ª–∏ ANTHROPIC_API_KEY –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è.")
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ AI —Å–µ—Ä–≤–∏—Å–æ–≤: {str(e)}")
        ai_manager = None
    
    print("üöÄ MCP Code Analyzer API —Å AI –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π –∑–∞–ø—É—â–µ–Ω!")
    print("üìñ –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è: http://localhost:8000/docs")
    print("ü§ñ AI —Å—Ç–∞—Ç—É—Å: http://localhost:8000/api/ai-status")

if __name__ == "__main__":
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info"
    )
